{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9f59e4e200>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jkr/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/jkr/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jkr/ml_notebooks/cifar10.ipynb Cell 1\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jkr/ml_notebooks/cifar10.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m train_dataset_gpu \u001b[39m=\u001b[39m {}\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jkr/ml_notebooks/cifar10.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m eval_dataset_gpu \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jkr/ml_notebooks/cifar10.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m train_dataset_gpu[\u001b[39m'\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m'\u001b[39m], train_dataset_gpu[\u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m [item\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mhyp[\u001b[39m'\u001b[39m\u001b[39mmisc\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m], non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_dataset_gpu_loader))]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jkr/ml_notebooks/cifar10.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m eval_dataset_gpu[\u001b[39m'\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m'\u001b[39m],  eval_dataset_gpu[\u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m=\u001b[39m [item\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mhyp[\u001b[39m'\u001b[39m\u001b[39mmisc\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m], non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(eval_dataset_gpu_loader)) ]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jkr/ml_notebooks/cifar10.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m cifar10_std, cifar10_mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstd_mean(train_dataset_gpu[\u001b[39m'\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m'\u001b[39m], dim\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)) \u001b[39m# dynamically calculate the std and mean from the data. this shortens the code and should help us adapt to new datasets!\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import functools\n",
    "from functools import partial\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "lt_conv_kwargs = {'kernel_size': 3, 'padding': 'same', 'bias': False}\n",
    "\n",
    "batchsize = 1024\n",
    "bias_scaler = 56\n",
    "\n",
    "hyp = {\n",
    "    'opt': {\n",
    "        'bias_lr':        1.64 * bias_scaler/512, # TODO: Is there maybe a better way to express the bias and batchnorm scaling? :'))))\n",
    "        'non_bias_lr':    1.64 / 512,\n",
    "        'bias_decay':     1.08 * 6.45e-4 * batchsize/bias_scaler,\n",
    "        'non_bias_decay': 1.08 * 6.45e-4 * batchsize,\n",
    "        'scaling_factor': 1./9,\n",
    "        'percent_start': .23,\n",
    "        'loss_scale_scaler': 1./128, # * Regularizer inside the loss summing (range: ~1/512 - 16+). FP8 should help with this somewhat too, whenever it comes out. :)\n",
    "    },\n",
    "    'net': {\n",
    "        'whitening': {\n",
    "            'kernel_size': 2,\n",
    "            'num_examples': 50000,\n",
    "        },\n",
    "        'batch_norm_momentum': .5, # * Don't forget momentum is 1 - momentum here (due to a quirk in the original paper... >:( )\n",
    "        'conv_norm_pow': 2.6,\n",
    "        'cutmix_size': 3,\n",
    "        'cutmix_epochs': 6,\n",
    "        'pad_amount': 2,\n",
    "        'base_depth': 64 ## This should be a factor of 8 in some way to stay tensor core friendly\n",
    "    },\n",
    "    'misc': {\n",
    "        'ema': {\n",
    "            'epochs': 10, # Slight bug in that this counts only full epochs and then additionally runs the EMA for any fractional epochs at the end too\n",
    "            'decay_base': .95,\n",
    "            'decay_pow': 3.,\n",
    "            'every_n_steps': 5,\n",
    "        },\n",
    "        'train_epochs': 12.1,\n",
    "        'device': 'cpu',\n",
    "        'data_location': 'data.pt',\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "if not os.path.exists(hyp['misc']['data_location']):\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor()])\n",
    "\n",
    "        cifar10      = torchvision.datasets.CIFAR10('cifar10/', download=True,  train=True,  transform=transform)\n",
    "        cifar10_eval = torchvision.datasets.CIFAR10('cifar10/', download=False, train=False, transform=transform)\n",
    "\n",
    "        # use the dataloader to get a single batch of all of the dataset items at once.\n",
    "        train_dataset_gpu_loader = torch.utils.data.DataLoader(cifar10, batch_size=len(cifar10), drop_last=True,\n",
    "                                                  shuffle=True, num_workers=2, persistent_workers=False)\n",
    "        eval_dataset_gpu_loader = torch.utils.data.DataLoader(cifar10_eval, batch_size=len(cifar10_eval), drop_last=True,\n",
    "                                                  shuffle=False, num_workers=1, persistent_workers=False)\n",
    "\n",
    "        train_dataset_gpu = {}\n",
    "        eval_dataset_gpu = {}\n",
    "\n",
    "        train_dataset_gpu['images'], train_dataset_gpu['targets'] = [item.to(device=hyp['misc']['device'], non_blocking=True) for item in next(iter(train_dataset_gpu_loader))]\n",
    "        eval_dataset_gpu['images'],  eval_dataset_gpu['targets']  = [item.to(device=hyp['misc']['device'], non_blocking=True) for item in next(iter(eval_dataset_gpu_loader)) ]\n",
    "\n",
    "        cifar10_std, cifar10_mean = torch.std_mean(train_dataset_gpu['images'], dim=(0, 2, 3)) # dynamically calculate the std and mean from the data. this shortens the code and should help us adapt to new datasets!\n",
    "\n",
    "        def batch_normalize_images(input_images, mean, std):\n",
    "            return (input_images - mean.view(1, -1, 1, 1)) / std.view(1, -1, 1, 1)\n",
    "\n",
    "        # preload with our mean and std\n",
    "        batch_normalize_images = partial(batch_normalize_images, mean=cifar10_mean, std=cifar10_std)\n",
    "\n",
    "        ## Batch normalize datasets, now. Wowie. We did it! We should take a break and make some tea now.\n",
    "        train_dataset_gpu['images'] = batch_normalize_images(train_dataset_gpu['images'])\n",
    "        eval_dataset_gpu['images']  = batch_normalize_images(eval_dataset_gpu['images'])\n",
    "\n",
    "        data = {\n",
    "            'train': train_dataset_gpu,\n",
    "            'eval': eval_dataset_gpu\n",
    "        }\n",
    "\n",
    "        ## Convert dataset to FP16 now for the rest of the process....\n",
    "        data['train']['images'] = data['train']['images'].half().requires_grad_(False)\n",
    "        data['eval']['images']  = data['eval']['images'].half().requires_grad_(False)\n",
    "\n",
    "        # Convert this to one-hot to support the usage of cutmix (or whatever strange label tricks/magic you desire!)\n",
    "        data['train']['targets'] = F.one_hot(data['train']['targets']).half()\n",
    "        data['eval']['targets'] = F.one_hot(data['eval']['targets']).half()\n",
    "\n",
    "        torch.save(data, hyp['misc']['data_location'])\n",
    "\n",
    "else:\n",
    "    ## This is effectively instantaneous, and takes us practically straight to where the dataloader-loaded dataset would be. :)\n",
    "    ## So as long as you run the above loading process once, and keep the file on the disc it's specified by default in the above\n",
    "    ## hyp dictionary, then we should be good. :)\n",
    "    data = torch.load(hyp['misc']['data_location'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
